[toc]

# 1.梯度下降法推导

## 目标函数：

$$
E(w)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})^2
$$

其中：
$$
\overline{y}^{(i)}=w^{T}x^{(i)}
$$
且：
$$
x^{(i)}=\left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ]  ，
w=\left[ \begin{matrix} w^0 \\ w^1 \\ \vdots \\ w^m \end{matrix} \right ],
y=\left[ \begin{matrix} y^0 \\ y^1 \\ \vdots \\ y^n \end{matrix} \right ]
$$
其中$n$的表示**共有$n$个样本**，$m$表示每**一个样本的维度**。

**注：**由于增加了bias，即$x_{(i)}^0$，所以导致样本的维度变为$m+1$维。

## 梯度下降公式：

$$
w_{new}=w_{old} - \eta \Delta E(w_{old})
$$

其中：

$\Delta$ 称作**梯度算子** ;
$\Delta E(w)$称作$E(w)$的**梯度**;
$\eta$ 为步长，也叫**学习速率**.

## $\Delta E(w_{old})$推导过程：

### 1.公式化简

$$
\Delta E(w_{old}) = \frac{\partial E(w_{old})}{\partial w_{old}} \\
=\frac{\partial \frac{1}{2}\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})^2}{\partial w_{old}} \\
=\frac{1}{2} \sum_{i=1}^n \frac{\partial (y^{(i)}-\overline{y}^{(i)})^2}{\partial w_{old}}
$$

分别计算累加项中的内容，最后求和即可。

### 2.当$i=1$时：

$$
\frac{\partial (y^{(1)}-\overline{y}^{(1)})^2}{\partial w_{old}} =\frac{\partial (y^{(1)}-w_{old}^{T}x^{(1)})^2}{\partial w_{old}} \\
=- 2(y^{(1)}-w_{old}^{T}x^{(1)})x^{(1)} \\
=- 2(y^{(1)}-\overline{y}^{(1)})x^{(1)}
$$

### 3.同理可得$i=2,3,4...n$时的值。

### 4.将结果进行累加求和：

$$
\Delta E(w_{old}) = - \sum_{i=1}^n \lbrace (y^{(i)}-\overline{y}^{(i)})x^{(i)} \rbrace \\
= - \sum_{i=1}^n \lbrace (y^{(i)}-\overline{y}^{(i)}) \left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ] \rbrace
$$

## 梯度下降公式重新整理：

$$
w_{new}=w_{old} - \eta \Delta E(w_{old}) \\
=w_{old} + \eta \sum_{i=1}^n \lbrace (y^{(i)}-\overline{y}^{(i)}) \left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ] \rbrace \\
=w_{old} + \eta \sum_{i=1}^n \lbrace (y^{(i)}-w_{old}^{T}x^{(i)}) \left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ] \rbrace \\
= w_{old} + \eta \sum_{i=1}^n \lbrace (y^{(i)}-\left[ \begin{matrix} w_{old}^0 & w_{old}^1 & \cdots & w_{old}^m \end{matrix} \right ] \left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ]) \left[ \begin{matrix} x_{(i)}^0 \\ x_{(i)}^1 \\ \vdots \\ x_{(i)}^m \end{matrix} \right ] \rbrace 
$$

## 个人总结：

重新推导梳理了一遍梯度下降公式，对该算法的设计思路有了新的认识。整体而言，梯度下降算法是对所有样本都进行了考虑过后计算得出的结果。对于一个样本而言，计算权重值改变后，使得该样本计算和目标结果损失最小的方向。将所有样本都进行该操作后，进行求和，从而得出权重改变的方向，这样就保证了少数偏离值不会对整体产生过大的影响。利用$\eta$ 来调节权重改变的大小，从而保证每一次的调节不会过大。

## 参考链接：

[零基础入门深度学习(2) - 线性单元和梯度下降](https://www.zybuluo.com/hanbingtao/note/448086)

# 2.Sigmoid函数

## 函数形式：

$$
sigmoid(x) = \frac{1}{1 + e^{-x}}
$$

## 导数形式：

$$
\dot{sigmoid(x)} = \frac {\partial sigmoid(x)}{\partial x} \\
= \frac {\partial (\frac{1}{1 + e^{-x}}) }{\partial x} \\
= - \frac{1}{(1 + e^{-x})^2} e^{-x}(-1) \\
= \frac{e^{-x}}{(1 + e^{-x})^2} \\
= sigmoid(x)(1-sigmoid(x))
$$

## 个人总结：

使用sigmoid函数，求的的结果可以有一个较好的输出，即结果都在0-1之间。并且该函数具有连续可导的性质，且导数结果较好使用。

# 3.反向传播

## 神经网络示意图：

![神经网络示意图](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/v2-5d352cb1e9cd3ec2c721908fea71c06f_720w.png)

## 前向传播过程：

### 计算$h_1$的输入$net_{h_1}$:

$$
net_{h_1} = w_1 \ast i_1 + w_2 \ast i_2 + 1 \ast b_1 \\
= 0.15 \ast 0.05 + 0.2 \ast 0.1 + 1 \ast 0.35 \\
\approx 0.3775
$$

### 计算$h_1$的输出$out_{h_1}$:

$$
out_{h_1} = sigmoid(net_{h_1}) \\
=\frac{1}{1-e^{-net_{h_1}}} \\
=\frac{1}{1-e^{-0.3775}} \\
\approx 0.5933
$$

同理计算$out_{h_2} \approx 0.5969$。

### 计算$o_1$的输入$net_{o_1}$:

$$
net_{o_1} = w_5 \ast out_{h_1} + w_6 \ast out_{h_2} + 1 \ast b_2 \\
= 0.4 \ast 0.5933 + 0.45 \ast 0.5969 + 1 \ast 0.6 \\
\approx 1.1059
$$

### 计算$o_1$的输出$out_{o_1}$:

$$
out_{o_1} = sigmoid(net_{o_1}) \\
=\frac{1}{1-e^{-net_{o_1}}} \\
=\frac{1}{1-e^{-1.1059}} \\
\approx 0.7514
$$

同理计算$out_{o_2} \approx 0.7729$

### 统计误差$E_{total}$：

$$
E_{total} = \sum \frac{1}{2} (target - output)^2 \\
= E_{o_1} + E_{o_2} \\
= \frac{1}{2} (0.01 - 0.7514)^2 + \frac{1}{2} (0.99 - 0.7729)^2 \\
\approx 0.2984
$$

## 反向传播过程：

### 输出层：

先计算$w_5$的偏导数，即$w_5$应该如何改变。
$$
\frac{\partial E_{total}}{\partial w_5} = \frac{\partial E_{total}}{\partial out_{o_1}} \frac{\partial out_{o_1}}{\partial net_{o_1}} \frac{\partial net_{o_1}}{\partial w_5} \\
=(out_{o_1}-target_{o_1}) \ast (out_{o_1}(1-out_{o_1})) \ast (out_{h_1}) \\
=(0.7514 - 0.01) \ast (0.7514(1-0.7514)) \ast(0.5933) \\
\approx 0.0822
$$
之后分别计算$w_6,w_7,w_8$的偏导数。

### 隐藏层：

先计算$w_1$的偏导数，即$w_1$应该如何改变。
$$
\frac{\partial E_{total}}{\partial w_1} = \frac{\partial E_{total}}{\partial out_{h_1}} \frac{\partial out_{h_1}}{\partial net_{h_1}} \frac{\partial net_{h_1}}{\partial w_1}
$$
其中，
$$
\frac{\partial E_{total}}{\partial out_{h_1}} = \frac{\partial E_{o_1}}{\partial out_{h_1}} + \frac{\partial E_{o_2}}{\partial out_{h_1}}
$$
分开计算，先计算$\frac{\partial E_{o_1}}{\partial out_{h_1}}$
$$
\frac{\partial E_{o_1}}{\partial out_{h_1}} = \frac{\partial E_{o_1}}{\partial out_{o_1}}\frac{\partial out_{o_1}}{\partial net_{o_1}}\frac{\partial net_{o_1}}{\partial out_{h_1}} \\
=(out_{o_1}-target_{o_1}) \ast (out_{o_1}(1-out_{o_1})) \ast (w_5)
$$
同理可得,
$$
\frac{\partial E_{o_2}}{\partial out_{h_1}}=(out_{o_2}-target_{o_2}) \ast (out_{o_2}(1-out_{o_2})) \ast (w_7)
$$
根据计算结果求得，
$$
\frac{\partial E_{total}}{\partial out_{h_1}}=(out_{o_1}-target_{o_1}) \ast (out_{o_1}(1-out_{o_1})) \ast (w_5) \\ + (out_{o_2}-target_{o_2}) \ast (out_{o_2}(1-out_{o_2})) \ast (w_7)
$$
由于,
$$
\frac{\partial out_{h_1}}{\partial net_{h_1}} \frac{\partial net_{h_1}}{\partial w_1}=(out_{h_1}(1-out_{h_1})) \ast (i_1)
$$
最后求得$\frac{\partial E_{total}}{\partial w_1}$。

## 个人总结：

反向传播是在正向传播计算结果的基础上进行的，根据误差总和对神经网络中的每一个权重值求偏导，从而将神经网络中的权重值进行调节。神经网络越是深层，隐藏层的权重值计算越是复杂。通过对公式的整理后，每一个权重值的偏微分又可以使用较为简单的，叠加的方式进行计算，利于实现[参考链接](https://www.zybuluo.com/hanbingtao/note/476663)。直接使用反向传播算法可以实现简单的**全连接(full connected, FC)**神经网络。

# 4.Relu激活函数

## 函数形式：

$$
f(x)=Max(0,x)
$$

## 函数图像：

![Relu](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/Relu.png)

## 相较于Sigmoid函数而言的优势：

### 速度快：

计算简单，没有指数、导数等计算。

### 减轻梯度消失问题：

利用反向传播计算梯度时，每一层梯度的计算值值都要在后面的基础上，再乘以一个关于sigmoid函数的导数，然而sigmoid函数本身导数小于0.25，所以导致越靠近输入层，其梯度计算结果越小，在深层的神经网络中，很可能会导致梯度消失的产生，即靠近输入层的参数改变极慢，靠近输出层的参数很快就变到了最优值。

### 稀疏性：

使用sigmoid函数时，工作时约有50%的神经元被激活，有人声称15-30%的激活率最优，使用Relu可以极大的降低神经元的激活率。

## 参考链接：

[零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480)

# 5.卷积神经网络（CNN）

## 网络简介：

卷积神经网络全称：Convolution Neural Network

### 卷积神经网络（CNN）和全联接（FC）：

全联接（FC）的缺点：参数众多，没有利用到像素点位置信息，网络层数受限（3层以内，否则梯度很难传递）。

卷积神经网络（CNN）的解决思路：局部链接（减少参数），权值共享（减少参数），下采样（Pooling层实现，减少每层样本数）。

### CNN整体流程示意图：

![CNNoverview](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNoverview.png)

### 一些重要的参数、名词以及理解：

图像的宽度、高度：图像的宽和高。

深度、Filter、通道（channel）：“卷积”操作使用的个数，得到的操作后图像个数。

卷积层、Polling层：完成卷积操作；使用**下采样**，获得更小的图像（Max polling，Mean polling等）。

Zero padding：在原始图像周围补0操作，按圈补。

步幅（step）：“卷积”操作时，每次移动的大小。

## 卷积层计算：

### 步幅为1时卷积层的计算过程：

![CNNconvolution](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNconvolution1.png)

卷积计算公式：
$$
a_{i,j}=f(\sum_{m=0}^{len}\sum_{n=0}^{high}w_{m,n}x_{i+m,j+n}+w_b)
$$
公式中：
$$
a_{i,j}:卷积操作后fearute \quad map中,对应i行j列的元素值。 \\
f：激活函数，本次使用relu作为激活函数。 \\
w_{m,n}:对应filter中，第m行n列位置的元素值，其大小也称为权重值。 \\
w_b:对应该filter的偏置项。 
$$
以上公式以及示例均为步幅为1时的结果。

### 步幅为2时卷积层的计算过程：

![CNNconvolution2](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNconvolution2.png)

计算公式相同，区别就是计算时每次移动的步幅改变，无论是横向移动还是纵向移动都进行了改变。

### 多个filter时的卷积层计算过程：

![CNNconvolution3](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNconvolution3.gif)

图示为2个filter时，卷积层的计算过程，此时zero padding为1，step为2，每一个filter为3*3的矩阵，并且每一个filter对应一个bias项。计算过程以及结果如图所示。

### 简化卷积层的计算，使用卷积公式进行表示：

**二维卷积公式**：
$$
C_{s,t}=\sum_0^{m_a-1}\sum_0^{n_a-1}A_{m,n}B_{s-m,t-n}
$$
公式中：
$$
矩阵A的行、列数：m_a,n_a \\
矩阵B的行、列数：m_b，n_b \\
0 \leq s < m_a+m_b-1 \\
0 \leq t < n_a+n_b-1
$$
上式可简写成：
$$
C=A*B
$$
**数学中的卷积和CNN中的卷积**：

数学中的卷积计算公式，和CNN中的卷积计算过程有区别，主要体现在相乘时位置的变换，如下图所示，所以在CNN中，为了避免命名混乱，所以将CNN中的卷积操作命名为**互相关**（cross-correlation）。

![CNNconvolution4](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNconvolution4.png)

**将CNN中的操作转化为数学公式中的卷积操作**：

将矩阵A翻转180度，交换A、B的位置，则数学公式中的卷积操作便转化为CNN中的**互相关**操作。如下式所示：
$$
A=f(\sum_{d=0}^{D-1}X_d*W_d+w_b)
$$

## Polling层的计算：

![CNNpolling](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/CNNpolling.png)

Polling层主要使用**下采样**的方式进行计算，主流使用的有Max polling、Mean polling等方式，并且针对深度为D的Feature map时，需要对每一层单独进行polling，最后也将得到D的深度。

## 个人总结：

CNN的思路不难理解，其主要利用了输入值位置间的关系，提供了除输入本身信息以外的其他信息；并利用了参数共享以及polling层的操作，减少了大量参数。然而在反向传播时，其推导过程因为“卷积”（互相关，cross-correlation）运算以及polling层的运算，导致过程较为复杂。以着实用为主的原则，了解其算法的原理与本质，以及可实现的方式，所以并未深度探索数学公式的推导。

# 6.循环神经网络（RNN）

## 网络简介：

循环神经网络全称（Recurrent neural network）

目的是为了处理**序列**而产生的神经网络，可以处理连续变化的数据，并考虑前后关系。

## 几种常见的循环神经网络：

### 基本循环神经网络：

网络示意图：

![RNNsimple](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/RNNsimple.jpg)

图中各个数值之间的关系：
$$
o_t=g(Vs_t) \\
s_t=f(Ux_t+Ws_{t-1})
$$
由公式可以看出，$o_t$的值受到前方$x_0,x_1,...,x_{t-1}$的影响，所以其可以向前看到无穷多个输入。

### 双向循环神经网络：

网络示意图：

![RNN2side](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/RNN2side.png)

图中数值关系：
$$
y_t=(Vs_t+V^`s^`_{i-t}) \\
s_t=f(Ux_{t-1}+Ws_{t-1}) \\
s^`_{i-t}=f(U^`x_t+W^`s^`_{i-t-1}) \\
其中V和V^`，U和U^`，W和W^`间权值不共享。
$$

### 深度循环网络：

网络示意图：

![RNNdeep](/Users/zhangxin/Desktop/Github/Deep-learning-example/img/RNNdeep.png)

网络中数值的计算：
$$
s^{(1)}_t=f(U^{(1)}x_t+W^{(1)}s_{t-1}^{(1)}) \\
s^{`(1)}_t=f(U^{`(1)}x_t+W^{`(1)}s_{t+1}^{`(1)}) \\
\dots \\
s^{(i)}_t=f(U^{(i)}s_t^{(i-1)}+W^{(i)}s_{t-1}^{(i-1)}) \\
s^{`(i)}_t=f(U^{`(i)}s_t^{(i-1)}+W^{`(i)}s_{t+1}^{`(i-1)}) \\
o_t=g(V^{(i)}s^{(i)}_t+V^{`(i)}s^{`(i)}_t) \\
式中，t时刻第i层隐藏层中的数值分别表示为s^{(i)}_t和s^{`(i)}_t。
$$
## 循环神经网络训练











